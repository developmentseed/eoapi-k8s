comment_install: >
  `service` and `gitSha` are required and defaulted value keys.
  a manual installation looks like this:

  $ export GITSHA=$(git rev-parse HEAD | cut -c1-10)
  $ helm install \
      --namespace eoapi \
      --create-namespace \
      --set gitSha=$GITSHA \
      eoapi \
      ./eoapi

# the chart on the gh-pages branch will provide
# the correct updated value otherwise it's defaulted
gitSha: "gitshaABC123"

#######################
# SERVICE ACCOUNT
#######################
serviceAccount:
  create: true
  name: ""
  automount: true
  annotations: {}
  labels: {}

######################
# SERVICE & INGRESS
######################
service:
  port: 8080

ingress:
  # Unified ingress configuration for both nginx and traefik
  enabled: true
  # ingressClassName: "nginx" or "traefik"
  className: "nginx"
  rootPath: ""        # Root path for doc server
  # Single host domain configuration (default)
  host: ""
  # Multiple host domains array - if specified, takes precedence over single host
  # hosts: []
  # Custom annotations to add to the ingress
  annotations: {}
  # TLS configuration
  tls:
    enabled: false
    secretName: eoapi-tls

######################
# DATABASE
######################
comment_db: >
  We use the crunchydata postgres operator/cluster charts as the k8s internal HA solution for eoapi.
  Those charts are therefore listed as a dependency of this chart in `Chart.yaml`.

  0. make sure to install the operator first `helm install --set disable_check_for_upgrades=true pgo oci://registry.developers.crunchydata.com/crunchydata/pgo`
  1. it will create a postgres cluster: see `postgrescluster.instances` spec below
  2. will will also create user credentials and mount their secrets: see `postgrescluster.users` spec below

  The `postgrescluster` specs below are pass-through values to configure those separate
  charts. For more information read https://access.crunchydata.com/documentation/postgres-operator/latest

# unified PostgreSQL configuration
postgresql:
  # Management type: "postgrescluster" (default), "external-plaintext", or "external-secret"
  type: "postgrescluster"

  # Configuration for external PostgreSQL (used when type is "external-plaintext" or "external-secret")
  external:
    # Connection information
    host: ""
    port: "5432"
    database: "eoapi"

    # Credentials configuration (used when type is "external-plaintext")
    credentials:
      username: ""
      password: ""

    # Secret reference (used when type is "external-secret")
    existingSecret:
      name: ""
      # Key mapping for the secret
      keys:
        username: "username"
        password: "password"
        # Optional: if these are provided in the secret
        # Note: These values override external.host, external.port and external.database if defined
        host: "host"
        port: "port"
        database: "database"

# this is declared as a dependency of eoapi in charts/eoapi/Chart.yaml
postgrescluster:
  enabled: true
  # The name of the postgres cluster
  # name: pgstac
  postgresVersion: 16
  postGISVersion: "3.4"
  pgBouncerReplicas: 1
  monitoring: false
  postgresClusterAnnotations: {}
  # Configure Patroni to set proper schema permissions
  patroni:
    dynamicConfiguration:
      postgresql:
        pg_hba:
          - "host all all 0.0.0.0/0 md5"
        parameters:
          shared_preload_libraries: pg_stat_statements, auto_explain
  databaseInitSQL:
    key: initdb.sql
    name: initdb
  instances:
    - name: eoapi
      replicas: 1
      dataVolumeClaimSpec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "10Gi"
            cpu: "1024m"
            memory: "3048Mi"
  # https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/user-management
  users:
    # `postgres` always has SUPERUSER privileges
    # we use this block here to create secrets for the `postgres` user that we can mount and lookup
    - name: postgres
      databases:
        - eoapi
        - postgres
      options: "SUPERUSER"
    # Grant CREATEROLE privilege to eoapi user so it can create the pgstac_admin role
    # If you grant SUPERUSER to the eoapi user, it won't be able to connect through pgbouncer
    # https://access.crunchydata.com/documentation/crunchy-postgres-containers/2.4.2/container-specifications/crunchy-pgbouncer/
    - name: eoapi
      databases:
        - eoapi
        - postgres
      options: "CREATEDB CREATEROLE"
      # default `password.type` is ASCII which follows the character set US-ASCII
      # but which can contain characters that `asyncpg` dislikes
      # see https://github.com/MagicStack/asyncpg/issues/1151
      password:
        type: AlphaNumeric

# `pgstacBootstrap` is a pod that by default runs pgstac schema migrations
# and optionally loads some fixtures for testing and examples
# using the LOAD_FIXTURES env var below
pgstacBootstrap:
  enabled: true
  image:
    name: ghcr.io/stac-utils/pgstac-pypgstac
    tag: v0.9.9
  configMaps:
    pgstacSettings:
      annotations:
        helm.sh/hook: "post-install,post-upgrade"
        helm.sh/hook-weight: "-7"
        helm.sh/hook-delete-policy: "before-hook-creation,hook-succeeded"
    initdbSql:
      annotations:
        helm.sh/hook: "post-install,post-upgrade"
        helm.sh/hook-weight: "-7"
        helm.sh/hook-delete-policy: "before-hook-creation,hook-succeeded"
    initdbJson:
      annotations:
        helm.sh/hook: "post-install,post-upgrade"
        helm.sh/hook-weight: "-7"
        helm.sh/hook-delete-policy: "before-hook-creation,hook-succeeded"
    pgstacQueryables:
      annotations:
        helm.sh/hook: "post-install,post-upgrade"
        helm.sh/hook-weight: "-7"
        helm.sh/hook-delete-policy: "before-hook-creation,hook-succeeded"
    initdb:
      annotations: {}
  jobs:
    pgstacSuperuserInitDb:
      annotations:
        helm.sh/hook: "post-install,post-upgrade"
        helm.sh/hook-weight: "-6"
        helm.sh/hook-delete-policy: "before-hook-creation"
    pgstacMigrate:
      annotations:
        helm.sh/hook: "post-install,post-upgrade"
        helm.sh/hook-weight: "-5"
        helm.sh/hook-delete-policy: "before-hook-creation"
    pgstacLoadSamples:
      annotations:
        helm.sh/hook: "post-install,post-upgrade"
        helm.sh/hook-weight: "-4"
        helm.sh/hook-delete-policy: "before-hook-creation"
    pgstacLoadQueryables:
      annotations:
        helm.sh/hook: "post-install,post-upgrade"
        helm.sh/hook-weight: "-3"
        helm.sh/hook-delete-policy: "before-hook-creation"


  settings:
    # General configuration options
    loadSamples: true      # Set to false to disable sample data loading

    # Queryables configuration
    # List of queryables configurations to load using pypgstac load-queryables
    # Each item requires a mandatory 'name' field and either 'file' or 'configMapRef'
    #
    # Example using files from the chart:
    # queryables:
    #   - name: "common-queryables.json"
    #     file: "data/initdb/queryables/common-queryables.json"
    #     indexFields: ["platform", "instruments"]
    #     deleteMissing: true
    #   - name: "collection-specific.json"
    #     file: "data/initdb/queryables/collection-specific.json"
    #     collections: ["my-collection-1", "my-collection-2"]
    #     indexFields: ["custom:field1", "custom:field2"]
    #     deleteMissing: true
    #
    # Example using external ConfigMap references:
    #   - name: "custom-queryables.json"
    #     configMapRef:
    #       name: my-custom-queryables-configmap
    #       key: queryables.json
    #     indexFields: ["custom:field1"]
    #     deleteMissing: true
    queryables: []

    # PgSTAC settings configuration
    # These settings control key PgSTAC behaviors and performance characteristics
    pgstacSettings:
      # Queue configuration for handling complex queries
      queue_timeout: "10 minutes"        # Timeout for queued queries (PostgreSQL interval format)
      use_queue: "false"                 # Enable/disable query queue mechanism

      # Collection extent management
      update_collection_extent: "true"   # Auto-update collection extents when items are added/modified

      # Context configuration for search results
      context: "auto"                    # Context mode: "on", "off", or "auto"
      context_estimated_count: "100000"  # Row threshold for using estimates instead of exact counts
      context_estimated_cost: "100000"   # Query cost threshold for using estimates
      context_stats_ttl: "1 day"         # Cache duration for context statistics

    # Queue processing configuration (only used when use_queue is "true")
    queueProcessor:
      schedule: "0 * * * *"               # Run every hour

    # Extent updater configuration (only used when update_collection_extent is "false")
    extentUpdater:
      schedule: "0 2 * * *"               # Run daily at 2 AM

    # Wait configuration for init containers waiting for pgstac jobs
    # These parameters control how long services wait for pgstac migration jobs to complete
    waitConfig:
      # Sleep interval between job status checks (in seconds)
      sleepInterval: 5
      # Maximum time to wait for jobs to complete (in seconds)
      # Default: 900 seconds (15 minutes)
      timeout: 900
    ### Database connection settings TEMPORARY UNTIL WE HAVE A BETTER SOLUTION
    ### FOR CONFIGURING THE DB CONNECTION
    user: eoapi
    database: eoapi
    resources: {}
    # DEPRECATED: Use pgstacBootstrap.settings.loadSamples instead
    envVars:
      # toggle to "false" if you don't want fixtures default loaded
      LOAD_FIXTURES: "true"

######################
# API SERVICES
######################
apiServices:
  - raster
  - multidim
  - stac
  - vector

raster:
  enabled: true
  ingress:
    enabled: true  # Control ingress specifically for raster service
    path: "/raster"  # Configurable path prefix for the raster service
  autoscaling:
    # NOTE: to have autoscaling working you'll need to enable monitoring
    # see ../../../docs/autoscaling.md for more information
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    # `type`: "cpu" || "requestRate" || "both"
    type: "requestRate"
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 60
      scaleUp:
        stabilizationWindowSeconds: 0
    targets:
      # matches `type` value above unless `type: "both"` is selected
      cpu: 75
      # 'm' units here represents generic milli (one-thousandth) unit instead of 'decimal'
      # https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#quantities
      # so when the average unit among these pods is <requestRate>/1000 then scale
      # you can watch the actual/target in real time using `kubectl get hpa/<name>`
      requestRate: 100000m
  image:
    name: ghcr.io/stac-utils/titiler-pgstac
    tag: 2.0.0
  command:
    - "uvicorn"
    - "titiler.pgstac.main:app"
    - "--host=$(HOST)"
    - "--port=$(PORT)"
  settings:
    labels: {}
    resources: {}
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    envVars:
      ##############
      # titiler
      ##############
      GDAL_CACHEMAX: "200"  # 200 mb
      GDAL_DISABLE_READDIR_ON_OPEN: "EMPTY_DIR"
      GDAL_INGESTED_BYTES_AT_OPEN: "32768"
      GDAL_HTTP_MERGE_CONSECUTIVE_RANGES: "YES"
      GDAL_HTTP_MULTIPLEX: "YES"
      GDAL_HTTP_VERSION: "2"
      PYTHONWARNINGS: "ignore"
      VSI_CACHE: "TRUE"
      VSI_CACHE_SIZE: "5000000"  # 5 MB (per file-handle)
      ##############
      # uvicorn
      ##############
      HOST: "0.0.0.0"
      PORT: "8080"
      # https://www.uvicorn.org/settings/#production
      WEB_CONCURRENCY: "4" # CPU-intensive; reduce to 2-3 with autoscaling
      DB_MIN_CONN_SIZE: "1"
      DB_MAX_CONN_SIZE: "3" # Less intensive: Metadata queries only

multidim:
  enabled: false # disabled by default
  ingress:
    enabled: true  # Control ingress specifically for multidim service
    path: "/multidim"  # Configurable path prefix for the multidim service
  autoscaling:
    # NOTE: to have autoscaling working you'll need to enable monitoring
    # see ../../../docs/autoscaling.md for more information
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    # `type`: "cpu" || "requestRate" || "both"
    type: "requestRate"
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 60
      scaleUp:
        stabilizationWindowSeconds: 0
    targets:
      # matches `type` value above unless `type: "both"` is selected
      cpu: 75
      # 'm' units here represents generic milli (one-thousandth) unit instead of 'decimal'
      # https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#quantities
      # so when the average unit among these pods is <requestRate>/1000 then scale
      # you can watch the actual/target in real time using `kubectl get hpa/<name>`
      requestRate: 100000m
  image:
    name: ghcr.io/developmentseed/titiler-md-demo
    tag: 6406b10406287ba9d3f345706d0c2be5f0265e02
  command:
    - "uvicorn"
    - "titiler_md_demo.main:app"
    - "--host=$(HOST)"
    - "--port=$(PORT)"
  settings:
    labels: {}
    resources: {}
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    envVars:
      ##############
      # titiler
      ##############
      GDAL_CACHEMAX: "200"  # 200 mb
      GDAL_DISABLE_READDIR_ON_OPEN: "EMPTY_DIR"
      GDAL_INGESTED_BYTES_AT_OPEN: "32768"
      GDAL_HTTP_MERGE_CONSECUTIVE_RANGES: "YES"
      GDAL_HTTP_MULTIPLEX: "YES"
      GDAL_HTTP_VERSION: "2"
      PYTHONWARNINGS: "ignore"
      VSI_CACHE: "TRUE"
      VSI_CACHE_SIZE: "5000000"  # 5 MB (per file-handle)
      ##############
      # uvicorn
      ##############
      HOST: "0.0.0.0"
      PORT: "8080"
      # https://www.uvicorn.org/settings/#production
      WEB_CONCURRENCY: "4" # CPU-intensive; reduce to 2-3 with autoscaling
      DB_MIN_CONN_SIZE: "1"
      DB_MAX_CONN_SIZE: "3" # Less intensive: Metadata queries only

stac:
  enabled: true
  ingress:
    enabled: true  # Control ingress specifically for stac service
    path: "/stac"  # Configurable path prefix for the stac service
  autoscaling:
    # NOTE: to have autoscaling working you'll need to enable monitoring
    # see ../../../docs/autoscaling.md for more information
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    # `type`: "cpu" || "requestRate" || "both"
    type: "requestRate"
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 60
      scaleUp:
        stabilizationWindowSeconds: 0
    targets:
      # matches `type` value above unless `type: "both"` is selected
      cpu: 75
      # 'm' units here represents generic milli (one-thousandth) unit instead of 'decimal'
      # https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#quantities
      # so when the average unit among these pods is <requestRate>/1000 then scale
      # you can watch the actual/target in real time using `kubectl get hpa/<name>`
      requestRate: 100000m
  image:
    name: ghcr.io/stac-utils/stac-fastapi-pgstac
    tag: 6.2.2
  command:
    - "uvicorn"
    - "stac_fastapi.pgstac.app:app"
    - "--host=$(HOST)"
    - "--port=$(PORT)"
  settings:
    labels: {}
    resources: {}
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    envVars:
      ##############
      # uvicorn
      ##############
      HOST: "0.0.0.0"
      PORT: "8080"
      # https://www.uvicorn.org/settings/#production
      WEB_CONCURRENCY: "10" # Handles many concurrent requests; reduce to 4-6 with autoscaling
      DB_MIN_CONN_SIZE: "1"
      DB_MAX_CONN_SIZE: "5" # Quite intensive (queries, transactions, searches)

# STAC Auth Proxy - authentication layer for STAC API
stac-auth-proxy:
  enabled: false
  image:
    tag: "v0.11.1"
  ingress:
    enabled: false  # Handled by main eoapi ingress
  service:
    port: 8080
  resources: {}
  env:
    # OIDC_DISCOVERY_URL must be configured when enabling auth (required)
    # UPSTREAM_URL must be set to "http://<release-name>-stac:8080" (required)
    ROOT_PATH: "/stac"
    OVERRIDE_HOST: "false"
    #
    # Authentication filters settings:
    DEFAULT_PUBLIC: "true" # This enables standard profile for authentication filters
    #
    # To use custom filters, ALL THREE of the following are required:
    # 1. Set filter class environment variables (uncomment lines below)
    # 2. Set customFiltersFile path (see below)
    # 3. Configure extraVolumes and extraVolumeMounts (see below)
    #
    # COLLECTIONS_FILTER_CLS: stac_auth_proxy.custom_filters:CollectionsFilter
    # ITEMS_FILTER_CLS: stac_auth_proxy.custom_filters:ItemsFilter

  # Path to custom filters file (relative to chart root)
  # Creates a ConfigMap from this file - required for custom filters
  # customFiltersFile: "data/stac-auth-proxy/custom_filters.py"

  # Volume referencing the ConfigMap - required for custom filters
  extraVolumes: []
  # Example (required for custom filters):
  # extraVolumes:
  #   - name: filters
  #     configMap:
  #       name: stac-auth-proxy-filters

  # Volume mount making filters available to container - required for custom filters
  extraVolumeMounts: []
  # Example (required for custom filters):
  # extraVolumeMounts:
  #   - name: filters
  #     mountPath: /app/src/stac_auth_proxy/custom_filters.py
  #     subPath: custom_filters.py
  #     readOnly: true

vector:
  enabled: true
  ingress:
    enabled: true  # Control ingress specifically for vector service
    path: "/vector"  # Configurable path prefix for the vector service
  autoscaling:
    # NOTE: to have autoscaling working you'll need to enable monitoring
    # see ../../../docs/autoscaling.md for more information
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    # `type`: "cpu" || "requestRate" || "both"
    type: "requestRate"
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 60
      scaleUp:
        stabilizationWindowSeconds: 0
    targets:
      # matches `type` value above unless `type: "both"` is selected
      cpu: 75
      # 'm' units here represents generic milli (one-thousandth) unit instead of 'decimal'
      # https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#quantities
      # so when the average unit among these pods is <requestRate>/1000 then scale
      # you can watch the actual/target in real time using `kubectl get hpa/<name>`
      requestRate: 100000m
  image:
    name: ghcr.io/developmentseed/tipg
    tag: 1.3.0
  command:
    - "uvicorn"
    - "tipg.main:app"
    - "--host=$(HOST)"
    - "--port=$(PORT)"
  settings:
    labels: {}
    resources: {}
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    envVars:
      ##############
      # tipg
      ##############
      TIPG_CATALOG_TTL: "300"
      TIPG_DEBUG: "True"
      ##############
      # uvicorn
      ##############
      HOST: "0.0.0.0"
      PORT: "8080"
      WEB_CONCURRENCY: "8" # Moderate concurrency for complex spatial queries; reduce to 4-5 with autoscaling
      DB_MIN_CONN_SIZE: "2"
      DB_MAX_CONN_SIZE: "5" # Vector queries can be complex and long-running

######################
# STAC Browser
######################
# It is a good idea to deploy stac-browser outside of k8s, since it's SPA with static files.
browser:
  enabled: true
  image:
    # we use a custom image with pathPrefix built into it
    name: ghcr.io/developmentseed/eoapi-k8s/stac-browser
    tag: 4.0.1
  ingress:
    enabled: true  # Control ingress specifically for browser service
    path: "/browser"
  # OAuth2 client ID for browser (frontend app)
  oidcClientId: "stac-browser"
  # OIDC discovery URL for browser (must be externally accessible URL)
  # Required when stac-auth-proxy is enabled
  oidcDiscoveryUrl: ""
  # Optional: Override the STAC catalog URL for the browser
  # If not set, will be constructed from ingress.host and stac.ingress.path
  # Useful when using custom ingress solutions with ingress.enabled=false
  catalogUrl: ""
  # Custom catalog title
  catalogTitle: ""
  # Custom catalog image/logo URL
  catalogImage: ""
  # Custom footer links
  footerLinks: []
  # Example:
  # footerLinks:
  #   - label: "Terms of use"
  #     url: "https://example.com/terms"
  #   - label: "Privacy"
  #     url: "https://example.com/privacy"
  settings:
    resources: {}

docServer:
  enabled: true
  settings:
    resources: {}

######################
# NOTIFICATIONS
######################
eoapi-notifier:
  enabled: false
  serviceAccount:
    name: ""
    create: false
  sources:
    - type: pgstac
      config:
        # Database connection from existing Kubernetes secret
        connection:
          existingSecret:
            name: ""
            keys:
              username: "user"
              password: "password"
              host: "host"
              port: "port"
              database: "dbname"

  # Outputs: Define where notifications are sent
  outputs:
    - type: mqtt
      config:
        broker_host: mqtt-broker
        broker_port: 1883
        # Optional: username, password, use_tls, topic, qos

    - type: cloudevents
      config:
        source: /eoapi/pgstac
        event_type: org.eoapi.stac.item
        # For KNative SinkBinding:
        destination:
          ref:
            apiVersion: messaging.knative.dev/v1
            kind: Broker
            name: my-channel-1
            namespace: serverless
            # For HTTP endpoints, use: endpoint: https://webhook.example.com

######################
# MOCK OIDC SERVER
######################
# Mock OIDC server for testing authentication (experimental profile only)
mockOidcServer:
  enabled: false

######################
# KNATIVE
######################
# Optional Knative components for CloudEvents and serverless workloads
knative:
  enabled: false
  version: "1.21"
  initTimeout: "600s"
  # CloudEvents sink configuration (deployed when eoapi-notifier uses CloudEvents output)
  cloudEventsSink:
    enabled: false
    image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.7
    resources: {}
    service:
      annotations:
        helm.sh/hook: "post-install,post-upgrade"
        helm.sh/hook-weight: "10"
        helm.sh/hook-delete-policy: "before-hook-creation"
    sinkBinding:
      annotations:
        helm.sh/hook: "post-install,post-upgrade"
        helm.sh/hook-weight: "30"
        helm.sh/hook-delete-policy: "before-hook-creation"
  jobs:
    init:
      annotations:
        helm.sh/hook: "post-install,post-upgrade"
        helm.sh/hook-weight: "0"
        helm.sh/hook-delete-policy: "before-hook-creation,hook-succeeded"

# Knative operator sub-chart configuration
knative-operator:
  tag: "v1.21.0"
  resources: {}

######################
# MONITORING
######################
# Core monitoring components for metrics collection and autoscaling
monitoring:
  # Metrics server - essential for HPA functionality
  metricsServer:
    enabled: false
    apiService:
      create: true

  # Prometheus - core metrics collection for autoscaling
  prometheus:
    enabled: false
    alertmanager:
      enabled: false
    prometheus-pushgateway:
      enabled: false
    kube-state-metrics:
      enabled: true
    prometheus-node-exporter:
      enabled: true
      resources: {}
    server:
      service:
        type: ClusterIP  # Internal service, no external exposure by default

  # Prometheus adapter - enables custom HPA metrics
  prometheusAdapter:
    enabled: false
    prometheus:
      # URL to Prometheus server - will be auto-configured for same-release Prometheus
      # If using external Prometheus, set this to your Prometheus URL
      # Example: http://my-prometheus-server.monitoring.svc.cluster.local
      url: http://eoapi-prometheus-server.eoapi.svc.cluster.local
      port: 80
      path: ""
    rules:
      default: false
      # Custom metrics for eoapi service autoscaling
      # Each service gets its own request rate metric for HPA scaling
      custom:
        # Vector service request rate metric
        - seriesQuery: '{__name__=~"^nginx_ingress_controller_requests$",namespace!=""}'
          seriesFilters: []
          resources:
            template: <<.Resource>>
          name:
            matches: ""
            as: "nginx_ingress_controller_requests_rate_vector_eoapi"
          metricsQuery: round(sum(rate(<<.Series>>{service="vector",path=~"/vector.*",<<.LabelMatchers>>}[5m])) by (<<.GroupBy>>), 0.001)

        # Raster service request rate metric
        - seriesQuery: '{__name__=~"^nginx_ingress_controller_requests$",namespace!=""}'
          seriesFilters: []
          resources:
            template: <<.Resource>>
          name:
            matches: ""
            as: "nginx_ingress_controller_requests_rate_raster_eoapi"
          metricsQuery: round(sum(rate(<<.Series>>{service="raster",path=~"/raster.*",<<.LabelMatchers>>}[5m])) by (<<.GroupBy>>), 0.001)

        # STAC service request rate metric
        - seriesQuery: '{__name__=~"^nginx_ingress_controller_requests$",namespace!=""}'
          seriesFilters: []
          resources:
            template: <<.Resource>>
          name:
            matches: ""
            as: "nginx_ingress_controller_requests_rate_stac_eoapi"
          metricsQuery: round(sum(rate(<<.Series>>{service="stac",path=~"/stac.*",<<.LabelMatchers>>}[5m])) by (<<.GroupBy>>), 0.001)

######################
# OBSERVABILITY
######################
# Grafana dashboards and visualization (requires monitoring.prometheus.enabled=true)
observability:
  grafana:
    enabled: false
    persistence:
      enabled: false
    service:
      type: LoadBalancer
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
        service.beta.kubernetes.io/aws-load-balancer-internal: "false"
    resources: {}
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Prometheus
            type: prometheus
            url: "http://{{ .Release.Name }}-prometheus-server"
            access: proxy
            isDefault: true
    dashboardsConfigMaps:
      default: "{{ .Release.Name }}-dashboards"

# Metrics Server sub-chart configuration
# These values are passed directly to the metrics-server sub-chart
metrics-server:
  apiService:
    create: true
